{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f3537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = \"changed the website html for security reasons",
    "\n",
    "response = requests.get(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705af1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Define the URL of the website to scrape\n",
    "url = 'changed the website html for security reasons\n",
    "\n",
    "# Define the pattern you want to match in href attributes\n",
    "pattern = '/kiado_alberlet/budapest'\n",
    "\n",
    "# Initialize a set to store unique href values\n",
    "unique_hrefs = set()\n",
    "\n",
    "# Initialize a list to store data\n",
    "data = []\n",
    "\n",
    "# Loop through pages until there are no more \"Következő\" links\n",
    "page = 1\n",
    "while True:\n",
    "    # Construct the URL for the current page\n",
    "    current_url = changed the website html for security reasons\n",
    "\n",
    "\n",
    "    # Send a GET request to the current page\n",
    "    response = requests.get(current_url)\n",
    "\n",
    "    # Check if the response status code indicates success\n",
    "    if response.status_code == 200:\n",
    "        # Parse the page content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all <a> tags with href attributes matching the pattern\n",
    "        matching_links = [a for a in soup.find_all('a', href=True) if pattern in a['href']]\n",
    "\n",
    "        for link in matching_links:\n",
    "            href_value = link['href']\n",
    "            if href_value not in unique_hrefs:\n",
    "                unique_hrefs.add(href_value)\n",
    "                print(\"Href:\", href_value)\n",
    "\n",
    "            # Extract and print the prices of matching links\n",
    "            div_col = link.find('div', class_='col')\n",
    "            if div_col is not None:\n",
    "                price_span = div_col.find('span', class_='price')\n",
    "                if price_span is not None:\n",
    "                    price = price_span.get_text(strip=True)\n",
    "                    print(\"Price:\", price)\n",
    "                    data.append([href_value, price])\n",
    "\n",
    "        # Check if there's a \"Következő\" link\n",
    "        next_page_link = soup.find('a', class_='pager_link', title='következő')\n",
    "        if next_page_link:\n",
    "            # Increment the page number for the next iteration\n",
    "            page += 1\n",
    "        else:\n",
    "            # No more pages to scrape, exit the loop\n",
    "            break\n",
    "    else:\n",
    "        # Handle request errors, such as retries or logging\n",
    "        print(f'Error: {response.status_code}')\n",
    "        break\n",
    "        \n",
    "    delay_seconds = random.uniform(5, 10)\n",
    "    print(f\"Waiting for {delay_seconds:.2f} seconds before the next scrape...\")\n",
    "    time.sleep(delay_seconds)\n",
    "    \n",
    "# Save the data to a CSV file\n",
    "with open('prices.csv', 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    # Write headers\n",
    "    csvwriter.writerow(['Href', 'Price'])\n",
    "    # Write data rows\n",
    "    csvwriter.writerows(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
